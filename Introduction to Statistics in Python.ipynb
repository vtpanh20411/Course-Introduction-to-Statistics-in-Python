{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Statistics in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Summary Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean and Media**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# Subset for Belgium and USA only\n",
    "be_and_usa = food_consumption[(food_consumption['country'] == 'Belgium') | (food_consumption['country'] == 'USA')]\n",
    "\n",
    "# Group by country, select consumption column, and compute mean and median\n",
    "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .agg() to calculate the mean and median of co2_emission for rice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for food_category equals rice\n",
    "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
    "\n",
    "# Calculate mean and median of co2_emission with .agg()\n",
    "print(rice_consumption['co2_emission'].agg([np.mean, np.median]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the quartiles of the co2_emission column of food_consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the quartiles of co2_emission\n",
    "print(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,4)))\n",
    "\n",
    "# Calculate the quartiles of co2_emission - Solution 2\n",
    "print(np.quantile(food_consumption['co2_emission'], [0,0.25,0.5,0.75,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance and Standard Deviation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the variance and standard deviation of `co2_emission` for each `food_category` by grouping and aggregating.\n",
    "- Import `matplotlib.pyplot` with alias `plt`.\n",
    "- Create a histogram of `co2_emission` for the `beef` `food_category` and show the plot.\n",
    "- Create a histogram of `co2_emission` for the `eggs` `food_category` and show the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print variance and sd of co2_emission for each food_category\n",
    "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'beef'\n",
    "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'eggs'\n",
    "food_consumption[food_consumption['food_category'] == 'egg']['co2_emission'].hist()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding outliers using IQR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total co2_emission per country: emissions_by_country\n",
    "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
    "\n",
    "# Compute the first and third quartiles and IQR of emissions_by_country\n",
    "q1 = np.quantile(emissions_by_country, 0.25)\n",
    "q3 = np.quantile(emissions_by_country, 0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate the lower and upper cutoffs for outliers\n",
    "lower = q1 - 1.5*iqr\n",
    "upper = q3 + 1.5*iqr\n",
    "\n",
    "# Subset emissions_by_country to find outliers\n",
    "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Random Numbers and Probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the deals for each product\n",
    "counts = amir_deals['product'].value_counts()\n",
    "print(counts)\n",
    "\n",
    "# Calculate probability of picking a deal with each product\n",
    "probs = counts / amir_deals.shape[0]\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample of 5 deals without replacement and save as sample_with_replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(24)\n",
    "\n",
    "# Sample 5 deals without replacement\n",
    "sample_without_replacement = amir_deals.sample(5)\n",
    "print(sample_without_replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample of 5 deals with replacement and save as sample_with_replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(24)\n",
    "\n",
    "# Sample 5 deals with replacement\n",
    "sample_with_replacement = amir_deals.sample(5, replace = True)\n",
    "print(sample_with_replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Numbers and Probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creating a probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of restaurant_groups and show plot\n",
    "restaurant_groups['group_size'].hist(bins = [2, 3, 4, 5, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability distribution\n",
    "size_dist = restaurant_groups['group_size'] / restaurant_groups.shape[0]\n",
    "\n",
    "# Reset index and rename columns\n",
    "size_dist = size_dist.reset_index()\n",
    "size_dist.columns = ['group_size', 'prob']\n",
    "\n",
    "print(size_dist)\n",
    "\n",
    "# Calculate expected value\n",
    "expected_value = np.sum(size_dist['group_size']*size_dist['prob'])\n",
    "print(expected_value)\n",
    "\n",
    "# Subset groups of size 4 or more\n",
    "groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
    "\n",
    "# Sum the probabilities of groups_4_or_more\n",
    "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
    "print(prob_4_or_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Continuous Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min and max wait times for back-up that happens every 30 min\n",
    "min_time = 0\n",
    "max_time = 30\n",
    "\n",
    "# Import uniform from scipy.stats\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Calculate probability of waiting less than 5 mins\n",
    "prob_less_than_5 = uniform.cdf(5, min_time, max_time)\n",
    "print(prob_less_than_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give Amir a better idea of how long he'll have to wait, you'll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\n",
    "\n",
    "As usual, `pandas` as `pd`, `numpy` as `np`, and `matplotlib.pyplot` as `plt` are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to 334\n",
    "np.random.seed(334)\n",
    "\n",
    "# Import uniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Generate 1000 wait times between 0 and 30 mins\n",
    "wait_times = uniform.rvs(0, 30, size=1000)\n",
    "\n",
    "# Create a histogram of simulated times and show plot\n",
    "plt.hist(wait_times)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binomial Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate a typical week of Amir's deals, or one week of 3 deals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import binom from scipy.stats\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Set random seed to 10\n",
    "np.random.seed(10)\n",
    "\n",
    "# Simulate 1 week of 3 deals\n",
    "print(binom.rvs(3, 0.3, size = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import binom from scipy.stats\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Set random seed to 10\n",
    "np.random.seed(10)\n",
    "\n",
    "# Simulate 52 weeks of 3 deals\n",
    "deals = binom.rvs(3, 0.3, size = 52)\n",
    "\n",
    "# Print mean deals won per week\n",
    "print(np.mean(deals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating binomial probabilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you'll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\n",
    "\n",
    "`binom` is imported from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of closing 3 out of 3 deals\n",
    "prob_3 = binom.pmf(3, 3, 0.3)\n",
    "\n",
    "print(prob_3)\n",
    "\n",
    "# Probability of closing <= 1 deal out of 3 deals\n",
    "prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)\n",
    "\n",
    "print(prob_less_than_or_equal_1)\n",
    "\n",
    "# Probability of closing > 1 deal out of 3 deals\n",
    "prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)\n",
    "\n",
    "print(prob_greater_than_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many sales will be won?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by nxp.\n",
    "\n",
    "- Calculate the expected number of sales out of the **3** he works on that Amir will win each week if he maintains his 30% win rate.\n",
    "- Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate drops to 25%.\n",
    "- Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate rises to 35%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected number won with 30% win rate\n",
    "won_30pct = 3*0.3\n",
    "print(won_30pct)\n",
    "\n",
    "# Expected number won with 25% win rate\n",
    "won_25pct = 3*0.25\n",
    "print(won_25pct)\n",
    "\n",
    "# Expected number won with 35% win rate\n",
    "won_35pct = 3*0.35\n",
    "print(won_35pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. More Distributions and the Central Limit Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normal Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Amir's sales**\n",
    "Since each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the `amount` column of `amir_deals` As part of Amir's performance review, you want to be able to estimate the probability of him selling different amounts, but before you can do this, you'll need to determine what kind of distribution the `amount` variable follows.\n",
    "\n",
    "Both `pandas` as `pd` and `matplotlib.pyplot` as `plt` are loaded and `amir_deals` is available.\n",
    "\n",
    "- Create a histogram with 10 bins to visualize the distribution of the `amount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of amount with 10 bins and show plot\n",
    "amir_deals['amount'].hist(bins = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probabilities from the normal distribution**\n",
    "\n",
    "Since each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the `amount` column of `amir_deals` and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts.\n",
    "\n",
    "`norm` from `scipy.stats` is imported as well as `pandas` as `pd`. The DataFrame `amir_deals` is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of deal < 7500\n",
    "prob_less_7500 = norm.cdf(7500, 5000, 2000)\n",
    "\n",
    "print(prob_less_7500)\n",
    "\n",
    "# Probability of deal < 7500\n",
    "prob_less_7500 = norm.cdf(7500, 5000, 2000)\n",
    "\n",
    "print(prob_less_7500)\n",
    "\n",
    "# Probability of deal between 3000 and 7000\n",
    "prob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)\n",
    "\n",
    "print(prob_3000_to_7000)\n",
    "\n",
    "# Calculate amount that 25% of deals will be less than\n",
    "pct_25 = norm.ppf(0.25, 5000,2000)\n",
    "\n",
    "print(pct_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulating sales under new market conditions**\n",
    "\n",
    "The company's financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale's worth will increase by 30%. To see what Amir's sales might look like next quarter under these new market conditions, you'll simulate new sales amounts using the normal distribution and store these in the `new_sales` DataFrame, which has already been created for you.\n",
    "\n",
    "In addition, `norm` from `scipy.stats`, `pandas` as `pd`, and `matplotlib.pyplot` as `plt` are loaded.\n",
    "\n",
    "- Currently, Amir's average sale amount is $5000. Calculate what his new average amount will be if it increases by 20% and store this in `new_mean`.\n",
    "- Amir's current standard deviation is $2000. Calculate what his new standard deviation will be if it increases by 30% and store this in `new_sd`.\n",
    "- Create a variable called `new_sales`, which contains 36 simulated amounts from a normal distribution with a mean of `new_mean` and a standard deviation of `new_sd`.\n",
    "- Plot the distribution of the `new_sales` `amount`s using a histogram and show the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new average amount\n",
    "new_mean = 5000*1.2\n",
    "\n",
    "# Calculate new standard deviation\n",
    "new_sd = 2000*1.3\n",
    "\n",
    "# Simulate 36 new sales\n",
    "new_sales = norm.rvs(new_mean, new_sd, size = 36)\n",
    "\n",
    "# Create histogram and show\n",
    "plt.hist(new_sales)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Central Limit Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The CLT in action**\n",
    "\n",
    "The central limit theorem states that a sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from.\n",
    "\n",
    "In this exercise, you'll focus on the sample mean and see the central limit theorem in action while examining the `num_users` column of `amir_deals` more closely, which contains the number of people who intend to use the product Amir is selling.\n",
    "\n",
    "`pandas` as `pd`, `numpy` as `np`, and `matplotlib.pyplot` as `plt` are loaded and `amir_deals` is available.\n",
    "\n",
    "• Repeat this 100 times using a `for` loop and store as `sample_means`. This will take 100 different samples and calculate the mean of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to 104\n",
    "np.random.seed(104)\n",
    "\n",
    "sample_means = []\n",
    "# Loop 100 times\n",
    "for i in range(100):\n",
    "  # Take sample of 20 num_users\n",
    "  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n",
    "  # Calculate mean of samp_20\n",
    "  samp_20_mean = np.mean(samp_20)\n",
    "  # Append samp_20_mean to sample_means\n",
    "  sample_means.append(samp_20_mean)\n",
    "  \n",
    "# Convert to Series and plot histogram\n",
    "sample_means_series = pd.Series(sample_means)\n",
    "sample_means_series.hist()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The mean of means**\n",
    "\n",
    "You want to know what the average number of users (`num_users`) is per deal, but you want to know this number for the entire company so that you can see if Amir's deals have more or fewer users than the company's average deal. The problem is that over the past year, the company has worked on more than ten thousand deals, so it's not realistic to compile all the data. Instead, you'll estimate the mean by taking several random samples of deals, since this is much easier than collecting data from everyone in the company.\n",
    "\n",
    "`amir_deals` is available and the user data for all the company's deals is available in `all_deals`. Both `pandas` as `pd` and `numpy` as `np` are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to 321\n",
    "np.random.seed(321)\n",
    "\n",
    "sample_means = []\n",
    "# Loop 30 times to take 30 means\n",
    "for i in range(30):\n",
    "  # Take sample of size 20 from num_users col of all_deals with replacement\n",
    "  cur_sample = all_deals['num_users'].sample(20, replace = True)\n",
    "  # Take mean of cur_sample\n",
    "  cur_mean = np.mean(cur_sample)\n",
    "  # Append cur_mean to sample_means\n",
    "  sample_means.append(cur_mean)\n",
    "\n",
    "# Print mean of sample_means\n",
    "print(np.mean(sample_means))\n",
    "\n",
    "# Print mean of num_users in amir_deals\n",
    "print(np.mean(amir_deals['num_users']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Poisson Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tracking lead responses**\n",
    "\n",
    "Your company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you'll calculate probabilities of Amir responding to different numbers of leads.\n",
    "\n",
    "- Import `poisson` from `scipy.stats` and calculate the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import poisson from scipy.stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Probability of 5 responses\n",
    "prob_5 = poisson.pmf(5,4)\n",
    "\n",
    "print(prob_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amir's coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import poisson from scipy.stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Probability of 5 responses\n",
    "prob_coworker = poisson.pmf(5,5.5)\n",
    "\n",
    "print(prob_coworker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the probability that Amir responds to 2 or fewer leads in a day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import poisson from scipy.stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Probability of 2 or fewer responses\n",
    "prob_2_or_less = poisson.cdf(2, 4)\n",
    "\n",
    "print(prob_2_or_less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the probability that Amir responds to more than 10 leads in a day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import poisson from scipy.stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Probability of > 10 responses\n",
    "prob_over_10 = 1 - poisson.cdf(10, 4)\n",
    "\n",
    "print(prob_over_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Exponential Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling time between leads**\n",
    "\n",
    "To further evaluate Amir's performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, he responds to 1 request every 2.5 hours. In this exercise, you'll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response.\n",
    "\n",
    "• Import `expon` from `scipy.stats`. What's the probability it takes Amir less than an hour to respond to a lead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import expon from scipy.stats\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Print probability response takes < 1 hour\n",
    "print(expon.cdf(1, scale= 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• What's the probability it takes Amir more than 4 hours to respond to a lead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import expon from scipy.stats\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Print probability response takes > 4 hours\n",
    "print(1 - expon.cdf(4, scale = 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• What's the probability it takes Amir 3-4 hours to respond to a lead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import expon from scipy.stats\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Print probability response takes 3-4 hours\n",
    "print(expon.cdf(4, scale = 2.5) - expon.cdf(3, scale = 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Correlation and Experimental Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationships between variables**\n",
    "\n",
    "In this chapter, you'll be working with a dataset `world_happiness` containing results from the **[2019 World Happiness Report](https://worldhappiness.report/ed/2019/)**. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\n",
    "\n",
    "In this exercise, you'll examine the relationship between a country's life expectancy (`life_exp`) and happiness score (`happiness_score`) both visually and quantitatively. `seaborn` as `sns`, `matplotlib.pyplot` as `plt`, and `pandas` as `pd` are loaded and `world_happiness` is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot of happiness_score vs. life_exp and show\n",
    "sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Create scatterplot of happiness_score vs life_exp with trendline\n",
    "sns.lmplot(x = 'life_exp', y = 'happiness_score', data = world_happiness, ci = None)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Correlation between life_exp and happiness_score\n",
    "cor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n",
    "\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation caveats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****What can't correlation measure?****\n",
    "\n",
    "While the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it's far from perfect. In this exercise, you'll explore one of the caveats of the correlation coefficient by examining the relationship between a country's GDP per capita (`gdp_per_cap`) and happiness score.\n",
    "\n",
    "`pandas` as `pd`, `matplotlib.pyplot` as `plt`, and `seaborn` as `sns` are imported, and `world_happiness` is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of gdp_per_cap and life_exp\n",
    "sns.scatterplot(x = 'gdp_per_cap', y = 'life_exp', data = world_happiness)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Correlation between gdp_per_cap and life_exp\n",
    "cor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n",
    "\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Transforming variables****\n",
    "\n",
    "When variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. In this exercise, you'll perform a transformation yourself.\n",
    "\n",
    "`pandas` as `pd`, `numpy` as `np`, `matplotlib.pyplot` as `plt`, and `seaborn` as `sns` are imported, and `world_happiness` is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log_gdp_per_cap column\n",
    "world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n",
    "\n",
    "# Scatterplot of happiness_score vs. log_gdp_per_cap\n",
    "sns.scatterplot(x = 'log_gdp_per_cap', y = 'happiness_score', data = world_happiness)\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\n",
    "print(cor)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
